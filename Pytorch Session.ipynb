{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88cdcbd2",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a036d110",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "PyTorch is an open-source deep learning framework designed to simplify the process of building neural networks and machine learning models. With its dynamic computation graph, PyTorch allows developers to modify the network’s behavior in real-time, making it an excellent choice for both beginners and researchers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfc1b7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.1-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.1-cp311-cp311-win_amd64.whl.metadata (6.1 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\sv937cy\\onedrive - ey\\desktop\\riotinto\\rio\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sv937cy\\onedrive - ey\\desktop\\riotinto\\rio\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\sv937cy\\onedrive - ey\\desktop\\riotinto\\rio\\.venv\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sv937cy\\onedrive - ey\\desktop\\riotinto\\rio\\.venv\\lib\\site-packages (from torchvision) (11.2.1)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sv937cy\\onedrive - ey\\desktop\\riotinto\\rio\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.7.1-cp311-cp311-win_amd64.whl (216.1 MB)\n",
      "   ---------------------------------------- 0.0/216.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 5.2/216.1 MB 29.0 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 7.6/216.1 MB 19.6 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 10.5/216.1 MB 17.2 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 13.9/216.1 MB 16.8 MB/s eta 0:00:13\n",
      "   -- ------------------------------------- 14.2/216.1 MB 14.8 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 15.5/216.1 MB 12.3 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 16.8/216.1 MB 11.5 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 18.9/216.1 MB 11.2 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 21.2/216.1 MB 11.2 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 23.6/216.1 MB 11.3 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 26.5/216.1 MB 11.4 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 29.9/216.1 MB 11.8 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 33.3/216.1 MB 12.1 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 37.0/216.1 MB 12.6 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 39.6/216.1 MB 12.5 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 40.9/216.1 MB 12.2 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 42.7/216.1 MB 11.9 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 44.8/216.1 MB 11.8 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 47.2/216.1 MB 11.8 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 50.1/216.1 MB 11.9 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 53.0/216.1 MB 12.0 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 56.4/216.1 MB 12.1 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 60.0/216.1 MB 12.4 MB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 61.9/216.1 MB 12.2 MB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 62.4/216.1 MB 11.8 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 63.2/216.1 MB 11.5 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 64.5/216.1 MB 11.3 MB/s eta 0:00:14\n",
      "   ------------ --------------------------- 66.3/216.1 MB 11.2 MB/s eta 0:00:14\n",
      "   ------------ --------------------------- 67.9/216.1 MB 11.1 MB/s eta 0:00:14\n",
      "   ------------- -------------------------- 70.5/216.1 MB 11.1 MB/s eta 0:00:14\n",
      "   ------------- -------------------------- 73.1/216.1 MB 11.2 MB/s eta 0:00:13\n",
      "   -------------- ------------------------- 76.3/216.1 MB 11.3 MB/s eta 0:00:13\n",
      "   -------------- ------------------------- 79.4/216.1 MB 11.4 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 83.1/216.1 MB 11.6 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 86.8/216.1 MB 11.8 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 88.6/216.1 MB 11.7 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 89.7/216.1 MB 11.5 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 91.2/216.1 MB 11.4 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 93.3/216.1 MB 11.3 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 95.9/216.1 MB 11.4 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 98.8/216.1 MB 11.4 MB/s eta 0:00:11\n",
      "   ------------------ -------------------- 102.2/216.1 MB 11.5 MB/s eta 0:00:10\n",
      "   ------------------- ------------------- 105.6/216.1 MB 11.6 MB/s eta 0:00:10\n",
      "   ------------------- ------------------- 109.1/216.1 MB 11.7 MB/s eta 0:00:10\n",
      "   -------------------- ------------------ 111.4/216.1 MB 11.7 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 113.0/216.1 MB 11.6 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 115.1/216.1 MB 11.6 MB/s eta 0:00:09\n",
      "   --------------------- ----------------- 117.2/216.1 MB 11.5 MB/s eta 0:00:09\n",
      "   --------------------- ----------------- 119.8/216.1 MB 11.6 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 122.7/216.1 MB 11.6 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 125.6/216.1 MB 11.6 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 128.2/216.1 MB 11.7 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 130.3/216.1 MB 11.6 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 132.4/216.1 MB 11.6 MB/s eta 0:00:08\n",
      "   ------------------------ -------------- 135.3/216.1 MB 11.6 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 138.4/216.1 MB 11.7 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 139.7/216.1 MB 11.6 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 141.0/216.1 MB 11.5 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 142.9/216.1 MB 11.4 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 144.7/216.1 MB 11.4 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 146.8/216.1 MB 11.4 MB/s eta 0:00:07\n",
      "   --------------------------- ----------- 149.7/216.1 MB 11.4 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 152.8/216.1 MB 11.5 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 156.2/216.1 MB 11.5 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 159.9/216.1 MB 11.6 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 162.5/216.1 MB 11.6 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 164.6/216.1 MB 11.6 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 167.5/216.1 MB 11.6 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 169.3/216.1 MB 11.6 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 171.4/216.1 MB 11.6 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 173.5/216.1 MB 11.5 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 176.4/216.1 MB 11.6 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 178.5/216.1 MB 11.5 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 180.4/216.1 MB 11.5 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 182.7/216.1 MB 11.5 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 185.3/216.1 MB 11.5 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 188.0/216.1 MB 11.5 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 191.1/216.1 MB 11.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 192.7/216.1 MB 11.5 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 194.0/216.1 MB 11.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 195.8/216.1 MB 11.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 197.7/216.1 MB 11.4 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 200.3/216.1 MB 11.4 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 202.9/216.1 MB 11.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 205.8/216.1 MB 11.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 208.9/216.1 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/216.1 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  213.6/216.1 MB 11.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  215.2/216.1 MB 11.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.0/216.1 MB 11.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.0/216.1 MB 11.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.0/216.1 MB 11.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 216.1/216.1 MB 11.0 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.22.1-cp311-cp311-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 9.2 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 2.1/6.3 MB 10.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.7/6.3 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 11.4 MB/s eta 0:00:00\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 14.2 MB/s eta 0:00:00\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, fsspec, filelock, torch, torchvision\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.5.1 mpmath-1.3.0 networkx-3.5 sympy-1.14.0 torch-2.7.1 torchvision-0.22.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d8c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To installed in GPU \n",
    "## ! pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc47d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To installed in CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad5391b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\sv937cy\\onedrive - ey\\desktop\\riotinto\\rio\\.venv\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\sv937cy\\onedrive - ey\\desktop\\riotinto\\rio\\.venv\\lib\\site-packages (0.22.1)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.7.1-cp311-cp311-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\sv937cy\\onedrive - ey\\desktop\\riotinto\\rio\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\sv937cy\\onedrive - ey\\desktop\\riotinto\\rio\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\sv937cy\\onedrive - ey\\desktop\\riotinto\\rio\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\sv937cy\\onedrive - ey\\desktop\\riotinto\\rio\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sv937cy\\onedrive - ey\\desktop\\riotinto\\rio\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sv937cy\\onedrive - ey\\desktop\\riotinto\\rio\\.venv\\lib\\site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\sv937cy\\onedrive - ey\\desktop\\riotinto\\rio\\.venv\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sv937cy\\onedrive - ey\\desktop\\riotinto\\rio\\.venv\\lib\\site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sv937cy\\onedrive - ey\\desktop\\riotinto\\rio\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sv937cy\\onedrive - ey\\desktop\\riotinto\\rio\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torchaudio-2.7.1-cp311-cp311-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ------------------------------------- -- 2.4/2.5 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 11.9 MB/s eta 0:00:00\n",
      "Installing collected packages: torchaudio\n",
      "Successfully installed torchaudio-2.7.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a51751c",
   "metadata": {},
   "source": [
    "## Tensors in PyTorch\n",
    "A tensor is a multi-dimensional array that is the fundamental data structure used in PyTorch (and many other machine learning frameworks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9f09591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D Tensor (Vector):\n",
      "tensor([1, 2, 3])\n",
      "\n",
      "2D Tensor (Matrix):\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "Random Tensor (2x3):\n",
      "tensor([[0.0185, 0.3767, 0.5523],\n",
      "        [0.0106, 0.0924, 0.4034]])\n",
      "\n",
      "Zeros Tensor (2x3):\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Ones Tensor (2x3):\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor_1d = torch.tensor([1, 2, 3])\n",
    "print(\"1D Tensor (Vector):\")\n",
    "print(tensor_1d)\n",
    "print()\n",
    "\n",
    "tensor_2d = torch.tensor([[1, 2], [3, 4]])\n",
    "print(\"2D Tensor (Matrix):\")\n",
    "print(tensor_2d)\n",
    "print()\n",
    "\n",
    "random_tensor = torch.rand(2, 3)\n",
    "print(\"Random Tensor (2x3):\")\n",
    "print(random_tensor)\n",
    "print()\n",
    "\n",
    "zeros_tensor = torch.zeros(2, 3)\n",
    "print(\"Zeros Tensor (2x3):\")\n",
    "print(zeros_tensor)\n",
    "print()\n",
    "\n",
    "ones_tensor = torch.ones(2, 3)\n",
    "print(\"Ones Tensor (2x3):\")\n",
    "print(ones_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58b210f",
   "metadata": {},
   "source": [
    "## Tensor Operations in PyTorch\n",
    "PyTorch operations are essential for manipulating data efficiently, especially when preparing data for machine learning tasks.\n",
    "\n",
    "Indexing: Indexing lets you retrieve specific elements or smaller sections from a larger tensor.\\\n",
    "Slicing: Slicing allows you to take out a portion of the tensor by specifying a range of rows or columns.\\\n",
    "Reshaping: Reshaping changes the shape or dimensions of a tensor without changing its actual data. This means you can reorganize the tensor into a different size while keeping all the original values intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c30e8022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed Element (Row 1, Column 0): 3\n",
      "Sliced Tensor (First two rows): \n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Reshaped Tensor (2x3): \n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "element = tensor[1, 0]\n",
    "print(f\"Indexed Element (Row 1, Column 0): {element}\")  \n",
    "slice_tensor = tensor[:2, :]\n",
    "print(f\"Sliced Tensor (First two rows): \\n{slice_tensor}\")\n",
    "\n",
    "reshaped_tensor = tensor.view(2, 3)\n",
    "print(f\"Reshaped Tensor (2x3): \\n{reshaped_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631893e5",
   "metadata": {},
   "source": [
    "## Common Tensor Functions: Broadcasting, Matrix Multiplication, etc.\n",
    "PyTorch offers a variety of common tensor functions that simplify complex operations.\n",
    "\n",
    "Broadcasting allows for automatic expansion of dimensions to facilitate arithmetic operations on tensors of different shapes.\\\n",
    "Matrix multiplication enables efficient computations essential for neural network operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e6ac913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcasted Addition Result: \n",
      "tensor([[11, 22, 33],\n",
      "        [14, 25, 36]])\n",
      "Matrix Multiplication Result (tensor_a * tensor_a^T): \n",
      "tensor([[14, 32],\n",
      "        [32, 77]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor_a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "tensor_b = torch.tensor([[10, 20, 30]]) \n",
    "\n",
    "broadcasted_result = tensor_a + tensor_b \n",
    "print(f\"Broadcasted Addition Result: \\n{broadcasted_result}\")\n",
    "\n",
    "matrix_multiplication_result = torch.matmul(tensor_a, tensor_a.T)\n",
    "print(f\"Matrix Multiplication Result (tensor_a * tensor_a^T): \\n{matrix_multiplication_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e32f5c",
   "metadata": {},
   "source": [
    "## GPU Acceleration with PyTorch\n",
    "PyTorch facilitates GPU acceleration, enabling much faster computations, which is especially important in deep learning due to the extensive matrix operations involved. By transferring tensors to the GPU, you can significantly reduce training times and improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae8e28fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Result shape (moved to CPU for printing): torch.Size([10000, 10000])\n",
      "Current GPU memory usage:\n",
      "Allocated: 0.00 MB\n",
      "Cached: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "tensor_size = (10000, 10000)  \n",
    "a = torch.randn(tensor_size, device=device)  \n",
    "b = torch.randn(tensor_size, device=device)  \n",
    "\n",
    "c = a + b  \n",
    "\n",
    "print(\"Result shape (moved to CPU for printing):\", c.cpu().shape)\n",
    "\n",
    "print(\"Current GPU memory usage:\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(device) / (1024 ** 2):.2f} MB\")\n",
    "print(f\"Cached: {torch.cuda.memory_reserved(device) / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e2cd42",
   "metadata": {},
   "source": [
    "## How PyTorch Is Used in LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ee5915",
   "metadata": {},
   "source": [
    "PyTorch is a deep learning framework (developed by Meta) used to:\n",
    "\n",
    "1) Build, train, and deploy neural networks\n",
    "\n",
    "2) Handle tensors (multi-dimensional arrays) and automatic differentiation\n",
    "\n",
    "3) Enable GPU acceleration\n",
    "\n",
    "4) Support dynamic computation graphs (flexible model building)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fac234",
   "metadata": {},
   "source": [
    "## How PyTorch Is Used in LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b7b585",
   "metadata": {},
   "source": [
    "| LLM Component               | PyTorch Feature Used                       |\n",
    "| --------------------------- | ------------------------------------------ |\n",
    "| Token Embedding Layer       | `nn.Embedding`                             |\n",
    "| Transformer Block           | `nn.MultiheadAttention` + `nn.Linear`      |\n",
    "| Feed-Forward Networks (FFN) | `nn.Linear` + activation functions         |\n",
    "| Positional Encoding         | Custom tensor operations                   |\n",
    "| Language Modeling Head      | `nn.Linear`                                |\n",
    "| Loss Calculation            | `F.cross_entropy` or `nn.CrossEntropyLoss` |\n",
    "| Optimizer                   | `torch.optim.Adam` or `SGD`                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37df4e1d",
   "metadata": {},
   "source": [
    "This is the core logic behind transformer-based LLMs like GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1fb537a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 2.7496\n",
      "Step 50, Loss: 0.5437\n",
      "Step 100, Loss: 0.9032\n",
      "Step 150, Loss: 0.2974\n",
      "Step 200, Loss: 1.0577\n",
      "Step 250, Loss: 1.2239\n",
      "Step 300, Loss: 1.2115\n",
      "Step 350, Loss: 0.9874\n",
      "Step 400, Loss: 0.3451\n",
      "Step 450, Loss: 0.7072\n",
      "\n",
      "Generated text:\n",
      "w w we wot we woe we wh wchcith weitwe wel we pyel \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# 1. Sample text (you can replace with any text corpus)\n",
    "text = \"hello world, welcome to mini gpt model with pytorch\"\n",
    "\n",
    "# 2. Create vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "# 3. Encoding and decoding functions\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "def decode(l): return ''.join([itos[i] for i in l])\n",
    "\n",
    "# 4. Prepare dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "block_size = 8  # context length\n",
    "\n",
    "def get_batch():\n",
    "    i = random.randint(0, len(data) - block_size - 1)\n",
    "    x = data[i:i+block_size]\n",
    "    y = data[i+1:i+block_size+1]\n",
    "    return x.unsqueeze(1), y.unsqueeze(1)  # shape: (T, 1)\n",
    "\n",
    "# 5. Define MiniGPT model\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads=2, batch_first=True)\n",
    "        self.ffn = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)                # (B, T, E)\n",
    "        x, _ = self.attn(x, x, x)        # (B, T, E)\n",
    "        logits = self.ffn(x)             # (B, T, vocab_size)\n",
    "        return logits\n",
    "\n",
    "# 6. Instantiate model, optimizer\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = MiniGPT(vocab_size, embed_dim=32).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 7. Training loop\n",
    "for step in range(500):\n",
    "    x_batch, y_batch = get_batch()\n",
    "    x_batch, y_batch = x_batch.transpose(0,1).to(device), y_batch.transpose(0,1).to(device)\n",
    "\n",
    "    logits = model(x_batch)  # (B, T, vocab)\n",
    "    loss = F.cross_entropy(logits.view(-1, vocab_size), y_batch.view(-1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 8. Text Generation\n",
    "def generate(start_char=\"h\", length=50):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([[stoi[start_char]]], dtype=torch.long).to(device)\n",
    "    output = [stoi[start_char]]\n",
    "    \n",
    "    for _ in range(length):\n",
    "        logits = model(idx)\n",
    "        probs = F.softmax(logits[:, -1, :], dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        output.append(next_id.item())\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    return decode(output)\n",
    "\n",
    "# 9. Generate text\n",
    "print(\"\\nGenerated text:\")\n",
    "print(generate(\"w\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49ade78",
   "metadata": {},
   "source": [
    "| Concept                 | What It Does                                       |\n",
    "| ----------------------- | -------------------------------------------------- |\n",
    "| `nn.Embedding`          | Turns characters into dense vectors                |\n",
    "| `nn.MultiheadAttention` | Captures relationships between characters          |\n",
    "| `nn.Linear`             | Predicts next character from the current state     |\n",
    "| `F.cross_entropy`       | Loss between predicted and true next characters    |\n",
    "| `torch.multinomial`     | Sample next character from predicted probabilities |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138a7d84",
   "metadata": {},
   "source": [
    "Absolutely! Let's break down the **MiniGPT code** you ran — **line by line**, in **layman’s terms**, and help you understand:\n",
    "\n",
    "---\n",
    "\n",
    "# 🧠 What This Code Does:\n",
    "\n",
    "We are building a **very tiny version of GPT**, called **MiniGPT**, using only:\n",
    "\n",
    "* An embedding layer\n",
    "* A single multi-head attention layer\n",
    "* A linear (fully connected) layer\n",
    "\n",
    "This model learns to **predict the next character** in a small piece of text like:\n",
    "\n",
    "```\n",
    "Input: \"welcom\"\n",
    "Target: \"elcome\"\n",
    "```\n",
    "\n",
    "If trained well, you can use it to **generate new words** character by character.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧱 Section-by-Section Explanation\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 1. Dataset & Vocabulary Creation\n",
    "\n",
    "```python\n",
    "text = \"hello world, welcome to mini gpt model with pytorch\"\n",
    "chars = sorted(list(set(text)))  # Unique characters\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}  # char -> index\n",
    "itos = {i: ch for ch, i in stoi.items()}      # index -> char\n",
    "```\n",
    "\n",
    "✅ What it does:\n",
    "\n",
    "* Takes your small text\n",
    "* Finds all **unique characters** (a-z, space, comma, etc.)\n",
    "* Maps each character to a number (index)\n",
    "\n",
    "🧠 Why?\n",
    "Models don’t understand text directly — they understand **numbers**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2. Encode & Prepare Data\n",
    "\n",
    "```python\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "block_size = 8\n",
    "```\n",
    "\n",
    "✅ What it does:\n",
    "\n",
    "* Encodes the entire string like `h=3`, `e=4`, `l=7`, ...\n",
    "* Breaks it into small sequences of **8 characters (context window)**\n",
    "\n",
    "### 🔹 3. Get Training Batch\n",
    "\n",
    "```python\n",
    "def get_batch():\n",
    "    i = random.randint(0, len(data) - block_size - 1)\n",
    "    x = data[i:i+block_size]\n",
    "    y = data[i+1:i+block_size+1]\n",
    "    return x.unsqueeze(1), y.unsqueeze(1)\n",
    "```\n",
    "\n",
    "✅ What it does:\n",
    "\n",
    "* Picks a random segment like:\n",
    "  `x = \"welcome \"`\n",
    "  `y = \"elcome t\"`\n",
    "* Adds a batch dimension to make it shape `(T, 1)`\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 4. Model Definition: `MiniGPT`\n",
    "\n",
    "```python\n",
    "self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "self.attn = nn.MultiheadAttention(embed_dim, num_heads=2, batch_first=True)\n",
    "self.ffn = nn.Linear(embed_dim, vocab_size)\n",
    "```\n",
    "\n",
    "✅ What each layer does:\n",
    "\n",
    "| Layer                | Purpose                                                                   |\n",
    "| -------------------- | ------------------------------------------------------------------------- |\n",
    "| `Embedding`          | Turns each character ID into a **vector** (dense representation)          |\n",
    "| `MultiheadAttention` | Looks at context: \"What other letters are important for this prediction?\" |\n",
    "| `Linear`             | Converts vector into vocab size to predict next letter                    |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 5. Forward Pass (Prediction)\n",
    "\n",
    "```python\n",
    "x = self.embed(x)                # Shape becomes (Batch, Time, Embedding)\n",
    "x, _ = self.attn(x, x, x)        # Apply attention to the input\n",
    "logits = self.ffn(x)             # Predict next character\n",
    "```\n",
    "\n",
    "🧠 At this stage:\n",
    "\n",
    "* Each character becomes a **vector**\n",
    "* Attention helps the model \"focus\" on important letters\n",
    "* Output is a prediction for each next character\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 6. Training the Model\n",
    "\n",
    "```python\n",
    "for step in range(500):\n",
    "    logits = model(x_batch)\n",
    "    loss = F.cross_entropy(logits.view(-1, vocab_size), y_batch.view(-1))\n",
    "    ...\n",
    "```\n",
    "\n",
    "✅ Training Goal:\n",
    "\n",
    "* Predict the next character given the current ones\n",
    "* Minimize the difference between **predicted** and **actual** next characters\n",
    "* Improve every step\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 7. Text Generation\n",
    "\n",
    "```python\n",
    "def generate(start_char=\"h\", length=50):\n",
    "    ...\n",
    "```\n",
    "\n",
    "✅ What it does:\n",
    "\n",
    "* Starts with one character (e.g., \"w\")\n",
    "* Predicts next one → adds to input → predicts next → keeps going\n",
    "* You get something like:\n",
    "\n",
    "```\n",
    "w → we → wel → welc → welco → welcom → welcome ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Why Your Output Is Repetitive?\n",
    "\n",
    "```text\n",
    "w w we wot we woe we wh wchcith weitwe wel we pyel\n",
    "```\n",
    "\n",
    "➡️ **Reason**:\n",
    "\n",
    "* Training data is **too small**\n",
    "* Only trained for **500 steps**\n",
    "* Model capacity is **tiny** (32-dimensional vectors)\n",
    "* The model hasn't learned complex patterns yet\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ What You Can Do to Improve\n",
    "\n",
    "1. **Train longer**\n",
    "\n",
    "   * Increase `range(500)` to `range(2000)` or more\n",
    "2. **Use more text**\n",
    "\n",
    "   * Feed it a larger dataset (e.g., from a file or public dataset)\n",
    "3. **Stack more layers**\n",
    "\n",
    "   * Add another attention + linear layer\n",
    "4. **Add positional encoding**\n",
    "\n",
    "   * So it knows where in the sequence it is\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Summary\n",
    "\n",
    "| Part         | What It Does                         |\n",
    "| ------------ | ------------------------------------ |\n",
    "| `Embedding`  | Converts text to vector              |\n",
    "| `Attention`  | Helps model focus on context         |\n",
    "| `Linear`     | Predicts next character              |\n",
    "| `Training`   | Learns by reducing prediction error  |\n",
    "| `Generate()` | Creates text one character at a time |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d5da6",
   "metadata": {},
   "source": [
    "## How PyTorch Helps in Memory Management of LLMs\n",
    "Training or running inference on LLMs can require huge memory (RAM/VRAM), especially for:\n",
    "\n",
    "1) Large vocabularies\n",
    "\n",
    "2) Long input sequences\n",
    "\n",
    "3) Deep transformer layers\n",
    "\n",
    "| Optimization Technique                 | How PyTorch Helps                                                  |\n",
    "| -------------------------------------- | ------------------------------------------------------------------ |\n",
    "| ✅ **Mixed Precision (float16)**        | Use `torch.cuda.amp` to reduce memory by 2x                        |\n",
    "| ✅ **Gradient Checkpointing**           | Recompute activations to reduce memory usage                       |\n",
    "| ✅ **Quantization (int8, 4-bit)**       | Reduce model size 4x–8x via `torch.quantization` or `bitsandbytes` |\n",
    "| ✅ **TorchScript**                      | Compile model for efficient inference                              |\n",
    "| ✅ **Model Parallelism**                | Spread large LLM across multiple GPUs                              |\n",
    "| ✅ **Zero Redundancy Optimizer (ZeRO)** | Shard optimizer states across GPUs                                 |\n",
    "\n",
    "\n",
    "PyTorch provides several features and tools to optimize memory usage:\n",
    "\n",
    "### 1. Mixed Precision Training (float16 / bfloat16)\n",
    "Reduces memory usage by half using lower-precision floating point:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29b97819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SV937CY\\AppData\\Local\\Temp\\ipykernel_29660\\1392887738.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\SV937CY\\AppData\\Local\\Temp\\ipykernel_29660\\1392887738.py:10: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Enable mixed precision\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# assume x_batch and y_batch are input/output tensors\n",
    "x_batch, y_batch = get_batch()\n",
    "x_batch, y_batch = x_batch.transpose(0,1).to(device), y_batch.transpose(0,1).to(device)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "with autocast():  # Enable mixed precision\n",
    "    output = model(x_batch)                       # Pass actual input tensor\n",
    "    loss = F.cross_entropy(output.view(-1, vocab_size), y_batch.view(-1))  # Compute loss\n",
    "\n",
    "scaler.scale(loss).backward()  # Backward pass in FP16\n",
    "scaler.step(optimizer)         # Optimizer step\n",
    "scaler.update()                # Update scaler for next iteration\n",
    "optimizer.zero_grad()          # Reset gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c162c9f",
   "metadata": {},
   "source": [
    "###  2. Gradient Checkpointing\n",
    "Recomputes some forward activations instead of storing them — trade compute for memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b82ff7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0131, 0.0000, 0.0000, 0.0000, 0.0000, 0.2348, 0.0000, 0.0000, 0.7775,\n",
      "         0.0000],\n",
      "        [0.0000, 0.4335, 0.0000, 0.0000, 0.0000, 0.4982, 0.6340, 1.8039, 0.0000,\n",
      "         0.6003],\n",
      "        [0.2243, 1.0454, 1.7345, 0.5063, 0.0000, 0.0000, 0.0905, 0.0000, 0.4688,\n",
      "         0.0000],\n",
      "        [0.0750, 0.9282, 0.0000, 0.0000, 1.0636, 0.0000, 1.1022, 1.5848, 0.0000,\n",
      "         0.4528]], grad_fn=<CheckpointFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# Dummy model block\n",
    "class MyBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.relu(self.linear(x))\n",
    "\n",
    "# Wrap it in a model\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block = MyBlock()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # checkpoint the block\n",
    "        return checkpoint(self.block, x)\n",
    "\n",
    "# Initialize model and input\n",
    "model = MyModel()\n",
    "input_tensor = torch.randn(4, 10, requires_grad=True)  # Batch of 4\n",
    "\n",
    "# Forward pass using checkpoint\n",
    "output = model(input_tensor)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ff962a",
   "metadata": {},
   "source": [
    "### 3. Model Quantization (int8 or 4-bit)\n",
    "Converts model weights from 32-bit → 8-bit (or even 4-bit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bfdbbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "model_quant = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418c7fed",
   "metadata": {},
   "source": [
    "| Optimization Technique                 | Description                                                                                  | How PyTorch Helps                                                                                                                                                | Benefit                                             |\n",
    "| -------------------------------------- | -------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- |\n",
    "| ✅ **Mixed Precision (float16)**        | Use half-precision (16-bit) instead of full precision (32-bit) where safe                    | `torch.cuda.amp` enables automatic mixed precision training. It handles when to use `float16` and when to fall back to `float32`                                 | ⚡ Faster computation and 🧠 \\~2x less memory        |\n",
    "| ✅ **Gradient Checkpointing**           | Save memory by not storing all intermediate activations; recompute them during backward pass | `torch.utils.checkpoint` allows you to wrap parts of your model to delay computation                                                                             | 🧠 30–50% memory savings during training            |\n",
    "| ✅ **Quantization (int8, 4-bit)**       | Represent model weights with lower precision (e.g., int8 or even 4-bit)                      | PyTorch supports post-training quantization via `torch.quantization`. Also supports 4-bit models via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) | 🚀 Smaller models, faster inference                 |\n",
    "| ✅ **TorchScript**                      | Compile PyTorch models to a static graph                                                     | Use `torch.jit.script()` or `torch.jit.trace()` to convert dynamic PyTorch into TorchScript                                                                      | 🚀 Faster inference, compatible with C++ deployment |\n",
    "| ✅ **Model Parallelism**                | Split a large model across multiple GPUs when it can't fit into one                          | Manually partition layers or use libraries like `torch.distributed.pipeline.sync.Pipe`                                                                           | 🧠 Enables very large model training (e.g., GPT-3)  |\n",
    "| ✅ **ZeRO (Zero Redundancy Optimizer)** | Shard model parameters, gradients, and optimizer states across GPUs                          | PyTorch DeepSpeed and `torch.distributed.fsdp` (Fully Sharded Data Parallel) implement ZeRO stages                                                               | 🧠 Train 10x–100x larger models across GPUs         |\n",
    "| ✅ **Dynamic Graphs and Autograd**      | PyTorch's dynamic computation graph allows fine-grained control of memory and operations     | Built-in `autograd` engine for gradient computation; can manually delete unused tensors                                                                          | ⚙️ Custom memory-efficient training logic           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafd8d7b",
   "metadata": {},
   "source": [
    "### 1. TorchScript (for Inference Optimization) \n",
    "Converts your model to a static, optimized graph using torch.jit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2d1bfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchScript Output: tensor([[-0.1812,  0.4482, -0.1455, -0.2589,  0.0545]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Instantiate and trace\n",
    "model = SimpleModel()\n",
    "example_input = torch.randn(1, 10)\n",
    "\n",
    "# Convert to TorchScript\n",
    "traced_model = torch.jit.trace(model, example_input)\n",
    "traced_model.save(\"simple_model.pt\")  # Save for production\n",
    "\n",
    "# Load and run\n",
    "loaded_model = torch.jit.load(\"simple_model.pt\")\n",
    "output = loaded_model(torch.randn(1, 10))\n",
    "print(\"TorchScript Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47167058",
   "metadata": {},
   "source": [
    "### 2. Model Parallelism (Manual GPU Splitting)\n",
    "Split model layers across two GPUs (requires at least 2 GPUs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a530ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Ensure you have at least 2 GPUs\n",
    "device0 = torch.device(\"cuda:0\")\n",
    "device1 = torch.device(\"cuda:1\")\n",
    "\n",
    "# Split model across devices\n",
    "class SplitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.seq1 = nn.Linear(10, 20).to(device0)\n",
    "        self.seq2 = nn.Linear(20, 5).to(device1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device0)\n",
    "        x = torch.relu(self.seq1(x))\n",
    "        x = x.to(device1)\n",
    "        return self.seq2(x)\n",
    "\n",
    "model = SplitModel()\n",
    "input_tensor = torch.randn(4, 10)\n",
    "output = model(input_tensor)\n",
    "print(\"Model Parallel Output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0c166f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. ZeRO (Zero Redundancy Optimizer) via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64558ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp.wrap import wrap\n",
    "\n",
    "import os\n",
    "\n",
    "def fsdp_main(rank, world_size):\n",
    "    # 1. Initialize process group\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "    # 2. Set device\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "    # 3. Model & wrap with FSDP\n",
    "    class ToyModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(1000, 1000),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1000, 1000)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    model = ToyModel().to(rank)\n",
    "    fsdp_model = FSDP(model)  # Automatically shards params\n",
    "\n",
    "    # 4. Training step\n",
    "    optim = torch.optim.Adam(fsdp_model.parameters(), lr=1e-3)\n",
    "    input_tensor = torch.randn(32, 1000).to(rank)\n",
    "    target = torch.randn(32, 1000).to(rank)\n",
    "\n",
    "    for step in range(5):\n",
    "        output = fsdp_model(input_tensor)\n",
    "        loss = nn.MSELoss()(output, target)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        print(f\"[Rank {rank}] Step {step} Loss: {loss.item()}\")\n",
    "\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "# To launch: torch.multiprocessing.spawn\n",
    "if __name__ == \"__main__\":\n",
    "    world_size = torch.cuda.device_count()\n",
    "    import torch.multiprocessing as mp\n",
    "    mp.spawn(fsdp_main, args=(world_size,), nprocs=world_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5b4295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
